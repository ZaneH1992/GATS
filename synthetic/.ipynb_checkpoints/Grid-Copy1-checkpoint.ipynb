{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, logging.handlers\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import math\n",
    "\n",
    "# round_ = 1\n",
    "# lookahead = 1\n",
    "target_update = 10000\n",
    "w = 10\n",
    "h = 10\n",
    "life_cost = 0.05\n",
    "# env_name = 'Grid'\n",
    "# env_name = '%s_LA_%d_W%d_H%d_round%d' %(env_name,lookahead,w,h,round_)\n",
    "# logger = logging.getLogger()\n",
    "# file_name = './data/results_%s.log' %(env_name)\n",
    "# fh = logging.handlers.RotatingFileHandler(file_name)\n",
    "# fh.setLevel(logging.DEBUG)#no matter what level I set here\n",
    "# formatter = logging.Formatter('%(asctime)s:%(message)s')\n",
    "# fh.setFormatter(formatter)\n",
    "# logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "seed = 0\n",
    "number_action = 4\n",
    "dimension = 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv' or 'SNConv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, dimension=2, num_actions=number_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimension, 50)\n",
    "#         self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(50, num_actions)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "#         x = F.relu(self.fc2(x.view(x.size(0), -1)))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ed7060935fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpos_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "w_ = w-1\n",
    "h_ = 0\n",
    "h__ = h-1\n",
    "gamma = 0.99\n",
    "pos_rew = 1\n",
    "def recurs(imagine_,h,Q):\n",
    "    if h == 0:\n",
    "        val = Q(imagine_.unsqueeze(0).unsqueeze(0).type(dtype)).max(1)\n",
    "        return val[0], val[1][0]\n",
    "    else:\n",
    "        action = 0\n",
    "        for i in range(number_action):\n",
    "            imagine = torch.clone(imagine_)\n",
    "            if i == 0:\n",
    "                imagine[1] = torch.clone(torch.clamp(imagine_[1] - 1,0,h-1))\n",
    "                cum_return_ = gamma * recurs(imagine,h-1,Q)[0] + pos_rew * torch.all(torch.eq(imagine, torch.tensor([[w_,h_]],dtype=torch.int8))).type(torch.FloatTensor)\\\n",
    "                                                               - torch.all(torch.eq(imagine[1], torch.tensor([h__],dtype=torch.int8))).type(torch.FloatTensor) - life_cost\n",
    "                \n",
    "            if i == 1:\n",
    "                imagine[0] = torch.clone(torch.clamp(imagine_[0] - 1,0,w-1))\n",
    "                cum_return = gamma * recurs(imagine,h-1,Q)[0] + pos_rew * torch.all(torch.eq(imagine, torch.tensor([[w_,h_]],dtype=torch.int8))).type(torch.FloatTensor)\\\n",
    "                                                              - torch.all(torch.eq(imagine[1], torch.tensor([h__],dtype=torch.int8))).type(torch.FloatTensor) - life_cost\n",
    "                if cum_return > cum_return_:\n",
    "                    cum_return_ = torch.clone(cum_return)\n",
    "                    action = 1\n",
    "            if i == 2:\n",
    "                imagine[1] = torch.clone(torch.clamp(imagine_[1] + 1,0,h-1))\n",
    "                cum_return = gamma * recurs(imagine,h-1,Q)[0] + pos_rew * torch.all(torch.eq(imagine, torch.tensor([[w_,h_]],dtype=torch.int8))).type(torch.FloatTensor)\\\n",
    "                                                              - torch.all(torch.eq(imagine[1], torch.tensor([h__],dtype=torch.int8))).type(torch.FloatTensor) - life_cost\n",
    "                if cum_return > cum_return_:\n",
    "                    cum_return_ = torch.clone(cum_return)\n",
    "                    action = 2\n",
    "            if i == 3:\n",
    "                imagine[0] = torch.clone(torch.clamp(imagine_[0] + 1,0,w-1))\n",
    "                cum_return = gamma * recurs(imagine,h-1,Q)[0] + pos_rew * torch.all(torch.eq(imagine, torch.tensor([[w_,h_]],dtype=torch.int8))).type(torch.FloatTensor)\\\n",
    "                                                              - torch.all(torch.eq(imagine[1], torch.tensor([h__],dtype=torch.int8))).type(torch.FloatTensor) - life_cost\n",
    "                if cum_return > cum_return_:\n",
    "                    cum_return_ = torch.clone(cum_return)\n",
    "                    action = 3   \n",
    "#         print(cum_return_)            \n",
    "        return cum_return_, torch.tensor(action)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#action\n",
    "#  0\n",
    "#1   3\n",
    "#  2\n",
    "#         w\n",
    "# x=0,y=0    x=w,y=0    \n",
    "#                      h\n",
    "# x=0,y=h    x=w,y=h   \n",
    "\n",
    "for round_count in range(10):\n",
    "    round_ = round_count + 1\n",
    "    lookahead = 1\n",
    "    env_name = 'Grid'\n",
    "    env_name = '%s_LA_%d_W%d_H%d_round%d' %(env_name,lookahead,w,h,round_)\n",
    "\n",
    "    init_w = 0\n",
    "    init_h = h-1\n",
    "    tot_epis = 1000\n",
    "    epsilon = .1\n",
    "\n",
    "\n",
    "    Q = DQN(dimension,number_action).type(dtype) \n",
    "    Target_Q = DQN(dimension,number_action).type(dtype) \n",
    "    Q.apply(weights_init)\n",
    "    optimizer = optim.SGD(Q.parameters(), lr = .0001, momentum=0.9)\n",
    "\n",
    "\n",
    "    actions = torch.zeros((1),dtype=torch.int8)\n",
    "    states = torch.zeros((1,2),dtype=torch.int8)\n",
    "    # done = torch.zeros((1)).type(dtype) \n",
    "    # states = torch.tensor([init_w,init_h])\n",
    "    rewards = torch.zeros((1)).type(dtype) \n",
    "    tot_rew = torch.zeros((1)).type(dtype) \n",
    "    # next_states = torch.zeros((1,2),dtype=torch.int8)\n",
    "    e = 0\n",
    "    tot_time = 2100000\n",
    "    time = 0\n",
    "    while time < tot_time:\n",
    "        t = 0\n",
    "        goal = 0\n",
    "        cum_rew = 0\n",
    "        # Initiate the state\n",
    "        if time > 0:\n",
    "            states = torch.cat((states,torch.tensor([[0,0]],dtype=torch.int8)),0)\n",
    "        state = torch.clone(states[-1])\n",
    "        while t<100 and goal==0:\n",
    "            time = time+1\n",
    "            next_state =  torch.clone(state)\n",
    "            sample = random.random()\n",
    "            if sample < epsilon:\n",
    "                action = torch.tensor(random.randrange(number_action))\n",
    "            else:\n",
    "                action = recurs(state,lookahead,Q)[1]\n",
    "            if time == 1:\n",
    "                actions[0] = action.unsqueeze(0).type(dtype) \n",
    "            else:\n",
    "                actions = torch.cat((actions,action.unsqueeze(0).type(torch.int8)),0)\n",
    "            reward = pos_rew*torch.all(torch.eq(state, torch.tensor([[w_,h_]],dtype=torch.int8))).type(torch.FloatTensor)\\\n",
    "                    - torch.all(torch.eq(state[1], torch.tensor([h__],dtype=torch.int8))).type(torch.FloatTensor) - life_cost\n",
    "    #         print(state)\n",
    "            if reward != -1 * life_cost :\n",
    "                goal = 1\n",
    "#                 if reward == pos_rew - life_cost:\n",
    "#                     print('bingo:t:',t,state)\n",
    "\n",
    "            if time == 1:\n",
    "                rewards[0] = reward.unsqueeze(0)\n",
    "            else:\n",
    "                rewards = torch.cat((rewards,reward.unsqueeze(0)),0)\n",
    "            if action == 0:\n",
    "                next_state[1] = torch.clone(torch.clamp(state[1] - 1,0,h-1))\n",
    "            if action == 1:\n",
    "                next_state[0] = torch.clone(torch.clamp(state[0] - 1,0,w-1))\n",
    "            if action == 2:\n",
    "                next_state[1] = torch.clone(torch.clamp(state[1] + 1,0,h-1))\n",
    "            if action == 3:\n",
    "                next_state[0] = torch.clone(torch.clamp(state[0] + 1,0,w-1))\n",
    "    #         print(state.type(torch.int).numpy(),action.type(torch.int).numpy(),next_state.type(torch.int).numpy()) \n",
    "            cum_rew = cum_rew + gamma * torch.clone(reward)   \n",
    "            states = torch.cat((states,torch.clone(next_state.unsqueeze(0))),0)\n",
    "            state = torch.clone(next_state)\n",
    "\n",
    "\n",
    "    #         current_value = Q(state.unsqueeze(0).unsqueeze(0).type(dtype))[0][action]    \n",
    "    #         loss = current_value-reward.type(dtype)-(1-goal)*gamma*Target_Q(next_state.unsqueeze(0).unsqueeze(0).type(dtype)).max(1)[0].detach()\n",
    "    #         d_error = loss.clamp(-1, 1)[0]\n",
    "    #         optimizer.zero_grad()\n",
    "    #         current_value.backward(d_error.data)\n",
    "    #         optimizer.step()\n",
    "            if time>5000:\n",
    "                if time % 4 == 0:        \n",
    "                    choices = np.random.choice(time, batchsize)\n",
    "                    current_action = actions[choices].unsqueeze(1)\n",
    "                    current_rewards = rewards[choices].type(dtype)\n",
    "                    indecies = np.linspace(0,batchsize-1,batchsize,dtype=np.int32)\n",
    "                    current_nextstates = states[choices+1].unsqueeze(1).type(dtype)\n",
    "                    current_value = Q(states[choices].unsqueeze(1).type(dtype))[indecies,current_action.squeeze(1).type(torch.int32).numpy()]\n",
    "                    loss = current_value-current_rewards-(current_rewards== -1 * life_cost).type(dtype)*gamma*Target_Q(current_nextstates).max(1)[0].detach()\n",
    "                    d_error = loss.clamp(-1, 1)\n",
    "                    optimizer.zero_grad()\n",
    "                    current_value.backward(d_error.data)\n",
    "                    optimizer.step()\n",
    "            t = t+1\n",
    "\n",
    "            if time % target_update == 0:\n",
    "                Target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "\n",
    "        if len(tot_rew) == 1:\n",
    "            tot_rew[0] = cum_rew.unsqueeze(0).type(dtype)\n",
    "        else:\n",
    "            tot_rew = torch.cat((tot_rew,cum_rew.unsqueeze(0).type(dtype)),0) \n",
    "        if e%100==0:    \n",
    "            print('e:',e,'t:',time,'rew:',cum_rew.type(dtype).numpy())\n",
    "        e = e + 1\n",
    "    #     s = torch.zeros((1,2),dtype=torch.int32)\n",
    "    #     s[0,0] = 3\n",
    "    #     s[0,1] = 3\n",
    "    #     ss = torch.zeros((1,2),dtype=torch.int32)\n",
    "    #     ss[0,0] = 0\n",
    "    #     ss[0,1] = 0\n",
    "    #     print(Q(s.type(dtype))-Q(ss.type(dtype)))\n",
    "\n",
    "\n",
    "    fnam = './data/reward%s' %(env_name)\n",
    "    np.save(fnam,rewards.numpy())                          \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #                 for i2 in range(number_action):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #  0\n",
    "    #1   3\n",
    "    #  2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 100\n",
    "sampling = 1000\n",
    "# tot_time = 100000\n",
    "\n",
    "LA0 = np.zeros((2,int(tot_time/sampling )-bandwidth))\n",
    "lookahead = 0\n",
    "for r in range(2):\n",
    "    round_ = r+1\n",
    "    env_name = 'Grid'\n",
    "    env_name = '%s_LA_%d_W%d_H%d_round%d' %(env_name,lookahead,w,h,round_)\n",
    "\n",
    "    fnam = './data/reward%s.npy' %(env_name)\n",
    "    tot_3 = np.load(fnam)\n",
    "    total_3_sample= np.zeros(int(tot_time/sampling ))\n",
    "    total_3= np.zeros(int(tot_time/sampling )-bandwidth)\n",
    "    for i in range(int(tot_time/sampling)):\n",
    "        total_3_sample[i] = np.sum(tot_3[sampling*i:sampling*i+sampling])/sampling\n",
    "    for i in range(len(total_3_sample)-bandwidth):\n",
    "        total_3[i] = np.sum(total_3_sample[i:i+bandwidth])/bandwidth\n",
    "\n",
    "    LA0[r,:] = total_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA1 = np.zeros((1,int(tot_time/sampling )-bandwidth))\n",
    "lookahead = 1\n",
    "for r in range(1):\n",
    "    round_ = r+1\n",
    "    env_name = 'Grid'\n",
    "    env_name = '%s_LA_%d_W%d_H%d_round%d' %(env_name,lookahead,w,h,round_)\n",
    "    fnam = './data/reward%s.npy' %(env_name)\n",
    "    tot_3 = np.load(fnam)\n",
    "    total_3_sample= np.zeros(int(tot_time/sampling ))\n",
    "    total_3= np.zeros(int(tot_time/sampling )-bandwidth)\n",
    "    for i in range(int(tot_time/sampling)):\n",
    "        total_3_sample[i] = np.sum(tot_3[sampling*i:sampling*i+sampling])/sampling\n",
    "    for i in range(len(total_3_sample)-bandwidth):\n",
    "        total_3[i] = np.sum(total_3_sample[i:i+bandwidth])/bandwidth\n",
    "\n",
    "    LA1[r,:] = total_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = 15\n",
    "fram = np.linspace(1,tot_time,int(tot_time/sampling ),dtype=np.int32)[0:-bandwidth]\n",
    "plt.ticklabel_format(style='sci', axis='x')\n",
    "plt.ticklabel_format(axis='both', style='sci', scilimits=(-2,2))\n",
    "plt.legend(fontsize=fonts)\n",
    "\n",
    "# ax = sns.tsplot(data=LA0,time=fram,value='Value',color = 'b',condition = \"LA = 0\")\n",
    "ax = sns.tsplot(data=LA1,time=fram,value='Value',color = 'c',condition = \"LA = 1\")\n",
    "\n",
    "plt.xlabel(\"Number of steps\",fontsize=fonts, family = 'serif')\n",
    "plt.ylabel(\"Average reward per episode\",fontsize=fonts, family = 'serif')\n",
    "plt.title(\"%s\" %(env_name),fontsize=fonts, family = 'serif')\n",
    "# ax.set(xlabel='Number of steps', ylabel='Average reward per episode')\n",
    "\n",
    "# plt.savefig(\"Pong_reward_lambda_3_runs.png\",format='png', dpi=2000)\n",
    "# plt.savefig(\"Pong_reward_lambda_3_runs.eps\",format='eps', dpi=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.zeros((1,2),dtype=torch.int32)\n",
    "s[0,0] = 3\n",
    "s[0,1] = 3\n",
    "ss = torch.zeros((1,2),dtype=torch.int32)\n",
    "ss[0,0] = 0\n",
    "ss[0,1] = 0\n",
    "print(Q(s.type(dtype))-Q(ss.type(dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.zeros((1,2),dtype=torch.int32)\n",
    "s[0,0] = 3\n",
    "s[0,1] = 3\n",
    "Q(s.type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.zeros((1,2),dtype=torch.int32)\n",
    "s[0,0] = 0\n",
    "s[0,1] = 0\n",
    "Q(s.type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nextstates= states[choices+1].unsqueeze(1).type(dtype)\n",
    "Target_Q(current_nextstates).max(1)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,batchsize-1,batchsize,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,batchsize-1,batchsize,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(current_rewards==0)*gamma*Target_Q(current_nextstates).max(1)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_rewards[0]==life_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_rewards == -1 * life_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = np.random.choice(time, batchsize)\n",
    "current_action = actions[choices].unsqueeze(1)\n",
    "current_rewards = rewards[choices].type(dtype)\n",
    "indecies = np.linspace(0,batchsize-1,batchsize,dtype=np.int32)\n",
    "current_nextstates = states[choices+1].unsqueeze(1).type(dtype)\n",
    "current_value = Q(states[choices].unsqueeze(1).type(dtype))[indecies,current_action.squeeze(1).type(torch.int32).numpy()]\n",
    "loss = current_value-current_rewards-(current_rewards== -1 * life_cost).type(dtype)*gamma*Target_Q(current_nextstates).max(1)[0].detach()\n",
    "d_error = loss.clamp(-1, 1)\n",
    "optimizer.zero_grad()\n",
    "current_value.backward(d_error.data)\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
